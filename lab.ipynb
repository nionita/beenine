{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8984e2b4-8840-4914-b7d7-116ce35beef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import sys\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from bbnn_dataset import BBNNDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20966524-7c38-45bc-9775-942143824284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why do we need to scale the network score prediction? It seems that\n",
    "# this is necessary in order to keep weigths & biases small,\n",
    "# so we chose a scale of 1000 - see PRED_SCALE\n",
    "#\n",
    "# For the loss function we compare the sigmoid of the scores (pred vs target)\n",
    "# in order to focus more on smaller absolute scores\n",
    "# Then we need to stretch the sigmoid by the centipawn score, like:\n",
    "# for how big a score are we almost winning (sigmoid approaches to 1)?\n",
    "# Now: sigmoid(4) = 0.982\n",
    "# We want to have that win probability for a score of 600 cp\n",
    "# Then the stretch factor must be 1 / 150\n",
    "PRED_SCALE = 1000.0\n",
    "SCORE_SIGMOID_SCALE = 1.0 / 150.0\n",
    "\n",
    "# For the model:\n",
    "NUM_INPUTS = 384\n",
    "L1 = 32\n",
    "L2 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "986ab5fa-7a6b-427d-8d3f-cf5a10c321db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model - only test\n",
    "class BBNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input  = nn.Linear(NUM_INPUTS * 2, L1)\n",
    "        self.output = nn.Linear(L1, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        w  = self.input(x)\n",
    "        l0 = torch.clamp(w, 0.0, 1.0)\n",
    "        y  = self.output(l0)\n",
    "        return y * PRED_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9df2e8a3-a42c-40f6-84ee-ffddcf6f9dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss based on sigmoid of the scores\n",
    "# Add a small fraction of the absolute difference of the scores\n",
    "# CPFRAC = 0.0001\n",
    "\n",
    "def loss_fn(pred, y, batch_no = 0):\n",
    "    #score, outcome = y\n",
    "\n",
    "    wdl_eval_model  = (pred * SCORE_SIGMOID_SCALE).sigmoid()\n",
    "    wdl_eval_target = (y    * SCORE_SIGMOID_SCALE).sigmoid()\n",
    "\n",
    "    # mloss = (torch.abs(wdl_eval_target - wdl_eval_model).square() + torch.abs(pred - y) * CPFRAC).mean()\n",
    "    mloss = torch.abs(wdl_eval_target - wdl_eval_model).square().mean()\n",
    "    return mloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9166682-5aa6-4a04-a17c-627d509a2994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, dataloader, model, loss_fn, optimizer, train_pos):\n",
    "    print(f'Train on {device} with {train_pos} positions')\n",
    "    start = time.time()\n",
    "    train_inst = 0\n",
    "    train_loss = 0\n",
    "    batch_report = None\n",
    "    model.train()\n",
    "    batch_no = 0\n",
    "    for X, y in dataloader:\n",
    "        batch_no += 1\n",
    "        n = X.shape[0]\n",
    "        train_inst += n\n",
    "        if batch_report is None:\n",
    "            batch_report = int(200000 / n)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # print(f'Batch {batch_no}: {X} -> {y}')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y, batch_no)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * n\n",
    "\n",
    "        if batch_no % batch_report == 0:\n",
    "            tdiff = time.time() - start\n",
    "            ips = round(train_inst / tdiff)\n",
    "            nows = time.strftime('%X %x')\n",
    "            mloss = train_loss / train_inst\n",
    "            print(f\"loss: {mloss:>7f} [{train_inst:>7d}/{train_pos:>7d}] {nows}: {ips:>6d} samples/second\")\n",
    "\n",
    "    nows = time.strftime('%X %x')\n",
    "    mloss = train_loss / train_inst\n",
    "    print(f\"Epoch loss: {mloss:>7f} [{train_inst:>7d}/{train_inst:>7d}] {nows}\")\n",
    "    return train_inst, mloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1ee3f61-087d-4589-a43d-87d69559df3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(device, dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    test_inst = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        batch_no = 0\n",
    "        for X, y in dataloader:\n",
    "            batch_no += 1\n",
    "            n = X.shape[0]\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item() * n\n",
    "            test_inst += n\n",
    "\n",
    "    test_loss /= test_inst\n",
    "    print(f\"Test Error Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61bbe58b-8224-47d5-8a63-8d27f93cdf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(device, dataloader, model, num):\n",
    "    model.eval()\n",
    "    eval_inst = 0\n",
    "    with torch.no_grad():\n",
    "        batch_no = 0\n",
    "        for X in dataloader:\n",
    "            batch_no += 1\n",
    "            n = X.shape[0]\n",
    "            X = X.to(device)\n",
    "            pred = model(X)\n",
    "            print(f'Eval instances {eval_inst + 1} to {eval_inst + n}: {pred}')\n",
    "            eval_inst += n\n",
    "            if eval_inst >= num:\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1866cb6-c444-4c8b-b680-89759b54d4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda\n"
     ]
    }
   ],
   "source": [
    "train_dir = r'C:\\data\\extract\\2025\\beenine\\train'\n",
    "test_dir = r'C:\\data\\extract\\2025\\beenine\\test'\n",
    "batch_size = 256\n",
    "lr = 0.01\n",
    "weight_decay = 0.0\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device is {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c89c146e-813e-48ea-8047-471c1aed1cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBNN(\n",
      "  (input): Linear(in_features=768, out_features=32, bias=True)\n",
      "  (output): Linear(in_features=32, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BBNN()\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e92b3c1f-86e7-4067-94ce-5ba1e4d26702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2215a308-4bdb-468c-9eb7-0564f47e164b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4c8d722-def1-49b8-8e55-56abe77dc969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "training_data = BBNNDataset(train_dir)\n",
    "\n",
    "# Test data\n",
    "test_data = BBNNDataset(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1a1b4d8-2697-4a77-a37f-6496cfe798b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(\n",
    "        training_data,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=False\n",
    "    )\n",
    "test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a897ea1e-8546-4390-b61c-1549377b64d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open feature file C:\\data\\extract\\2025\\beenine\\test\\xaa-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\test\\xaa-targ.txt\n",
      "Test Error Avg loss: 0.089573 \n",
      "\n",
      "Initial test loss: 0.08957291672730446\n"
     ]
    }
   ],
   "source": [
    "test_loss = test(device, test_dataloader, model, loss_fn)\n",
    "print(f'Initial test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49114ee0-5e23-44f1-af55-662e4d7bc0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 from 3\n",
      "-------------------------------\n",
      "Train on cuda with 0 positions\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xab-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xab-targ.txt\n",
      "loss: 0.084377 [ 199936/      0] 12:43:21 02/08/25:  55988 samples/second\n",
      "loss: 0.092933 [ 399872/      0] 12:43:24 02/08/25:  55766 samples/second\n",
      "loss: 0.092004 [ 599808/      0] 12:43:28 02/08/25:  56136 samples/second\n",
      "loss: 0.091971 [ 799744/      0] 12:43:31 02/08/25:  56417 samples/second\n",
      "loss: 0.088257 [ 999680/      0] 12:43:35 02/08/25:  56620 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xac-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xac-targ.txt\n",
      "loss: 0.089984 [1199616/      0] 12:43:38 02/08/25:  56520 samples/second\n",
      "loss: 0.089425 [1399552/      0] 12:43:42 02/08/25:  56590 samples/second\n",
      "loss: 0.090557 [1599488/      0] 12:43:45 02/08/25:  56693 samples/second\n",
      "loss: 0.092122 [1799424/      0] 12:43:49 02/08/25:  56854 samples/second\n",
      "loss: 0.093699 [1999360/      0] 12:43:52 02/08/25:  56966 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xad-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xad-targ.txt\n",
      "loss: 0.095223 [2199296/      0] 12:43:56 02/08/25:  57067 samples/second\n",
      "loss: 0.095622 [2399232/      0] 12:43:59 02/08/25:  57162 samples/second\n",
      "loss: 0.096977 [2599168/      0] 12:44:03 02/08/25:  57277 samples/second\n",
      "loss: 0.098943 [2799104/      0] 12:44:06 02/08/25:  57384 samples/second\n",
      "loss: 0.101554 [2999040/      0] 12:44:09 02/08/25:  57532 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xae-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xae-targ.txt\n",
      "loss: 0.102912 [3198976/      0] 12:44:13 02/08/25:  57568 samples/second\n",
      "loss: 0.102532 [3398912/      0] 12:44:16 02/08/25:  57544 samples/second\n",
      "loss: 0.101155 [3598848/      0] 12:44:20 02/08/25:  57501 samples/second\n",
      "loss: 0.099521 [3798784/      0] 12:44:23 02/08/25:  57458 samples/second\n",
      "loss: 0.097229 [3998720/      0] 12:44:27 02/08/25:  57386 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xaf-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xaf-targ.txt\n",
      "loss: 0.095401 [4198656/      0] 12:44:31 02/08/25:  57253 samples/second\n",
      "loss: 0.092959 [4398592/      0] 12:44:34 02/08/25:  57145 samples/second\n",
      "loss: 0.090561 [4598528/      0] 12:44:38 02/08/25:  57019 samples/second\n",
      "loss: 0.088871 [4798464/      0] 12:44:42 02/08/25:  56847 samples/second\n",
      "loss: 0.087243 [4998400/      0] 12:44:45 02/08/25:  56808 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xag-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xag-targ.txt\n",
      "loss: 0.086531 [5198336/      0] 12:44:49 02/08/25:  56819 samples/second\n",
      "loss: 0.086180 [5398272/      0] 12:44:52 02/08/25:  56775 samples/second\n",
      "loss: 0.086282 [5598208/      0] 12:44:56 02/08/25:  56740 samples/second\n",
      "loss: 0.086300 [5798144/      0] 12:44:59 02/08/25:  56747 samples/second\n",
      "loss: 0.086380 [5998080/      0] 12:45:03 02/08/25:  56733 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xah-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xah-targ.txt\n",
      "loss: 0.088107 [6198016/      0] 12:45:06 02/08/25:  56779 samples/second\n",
      "loss: 0.089637 [6397952/      0] 12:45:10 02/08/25:  56797 samples/second\n",
      "loss: 0.090890 [6597888/      0] 12:45:13 02/08/25:  56791 samples/second\n",
      "loss: 0.090667 [6797824/      0] 12:45:17 02/08/25:  56755 samples/second\n",
      "loss: 0.090111 [6997760/      0] 12:45:21 02/08/25:  56738 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xai-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xai-targ.txt\n",
      "loss: 0.089318 [7197696/      0] 12:45:24 02/08/25:  56743 samples/second\n",
      "loss: 0.089109 [7397632/      0] 12:45:28 02/08/25:  56718 samples/second\n",
      "loss: 0.087959 [7597568/      0] 12:45:31 02/08/25:  56667 samples/second\n",
      "loss: 0.087290 [7797504/      0] 12:45:35 02/08/25:  56614 samples/second\n",
      "loss: 0.086471 [7997440/      0] 12:45:39 02/08/25:  56579 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xaj-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xaj-targ.txt\n",
      "loss: 0.085992 [8197376/      0] 12:45:42 02/08/25:  56556 samples/second\n",
      "loss: 0.085621 [8397312/      0] 12:45:46 02/08/25:  56533 samples/second\n",
      "loss: 0.085706 [8597248/      0] 12:45:49 02/08/25:  56536 samples/second\n",
      "loss: 0.085349 [8797184/      0] 12:45:53 02/08/25:  56513 samples/second\n",
      "loss: 0.085094 [8997120/      0] 12:45:57 02/08/25:  56495 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xak-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xak-targ.txt\n",
      "loss: 0.084848 [9197056/      0] 12:46:00 02/08/25:  56506 samples/second\n",
      "loss: 0.084615 [9396992/      0] 12:46:04 02/08/25:  56511 samples/second\n",
      "loss: 0.084261 [9596928/      0] 12:46:07 02/08/25:  56499 samples/second\n",
      "loss: 0.083251 [9796864/      0] 12:46:11 02/08/25:  56467 samples/second\n",
      "loss: 0.082539 [9996800/      0] 12:46:14 02/08/25:  56443 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xal-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xal-targ.txt\n",
      "loss: 0.081996 [10196736/      0] 12:46:18 02/08/25:  56421 samples/second\n",
      "loss: 0.081159 [10396672/      0] 12:46:22 02/08/25:  56377 samples/second\n",
      "Epoch loss: 0.081077 [10423404/10423404] 12:46:22 02/08/25\n",
      "Saved PyTorch Model State to save-sgd-0.pth\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\test\\xaa-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\test\\xaa-targ.txt\n",
      "Test Error Avg loss: 0.088034 \n",
      "\n",
      "200 seconds per epoch - 400 seconds remaining\n",
      "-------------------------------\n",
      "Epoch 2 from 3\n",
      "-------------------------------\n",
      "Train on cuda with 10423404 positions\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xab-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xab-targ.txt\n",
      "loss: 0.083384 [ 199936/10423404] 12:46:41 02/08/25:  57389 samples/second\n",
      "loss: 0.092165 [ 399872/10423404] 12:46:44 02/08/25:  56749 samples/second\n",
      "loss: 0.091341 [ 599808/10423404] 12:46:48 02/08/25:  56722 samples/second\n",
      "loss: 0.091392 [ 799744/10423404] 12:46:52 02/08/25:  56671 samples/second\n",
      "loss: 0.087712 [ 999680/10423404] 12:46:55 02/08/25:  56584 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xac-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xac-targ.txt\n",
      "loss: 0.089488 [1199616/10423404] 12:46:59 02/08/25:  56487 samples/second\n",
      "loss: 0.088968 [1399552/10423404] 12:47:02 02/08/25:  56300 samples/second\n",
      "loss: 0.090133 [1599488/10423404] 12:47:06 02/08/25:  56402 samples/second\n",
      "loss: 0.091729 [1799424/10423404] 12:47:09 02/08/25:  56389 samples/second\n",
      "loss: 0.093334 [1999360/10423404] 12:47:13 02/08/25:  56382 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xad-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xad-targ.txt\n",
      "loss: 0.094881 [2199296/10423404] 12:47:16 02/08/25:  56688 samples/second\n",
      "loss: 0.095301 [2399232/10423404] 12:47:20 02/08/25:  56736 samples/second\n",
      "loss: 0.096672 [2599168/10423404] 12:47:23 02/08/25:  56782 samples/second\n",
      "loss: 0.098654 [2799104/10423404] 12:47:27 02/08/25:  56795 samples/second\n",
      "loss: 0.101279 [2999040/10423404] 12:47:30 02/08/25:  56807 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xae-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xae-targ.txt\n",
      "loss: 0.102644 [3198976/10423404] 12:47:34 02/08/25:  56822 samples/second\n",
      "loss: 0.102275 [3398912/10423404] 12:47:37 02/08/25:  56788 samples/second\n",
      "loss: 0.100904 [3598848/10423404] 12:47:41 02/08/25:  56729 samples/second\n",
      "loss: 0.099275 [3798784/10423404] 12:47:44 02/08/25:  56658 samples/second\n",
      "loss: 0.096987 [3998720/10423404] 12:47:48 02/08/25:  56596 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xaf-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xaf-targ.txt\n",
      "loss: 0.095165 [4198656/10423404] 12:47:52 02/08/25:  56508 samples/second\n",
      "loss: 0.092727 [4398592/10423404] 12:47:55 02/08/25:  56481 samples/second\n",
      "loss: 0.090334 [4598528/10423404] 12:47:59 02/08/25:  56397 samples/second\n",
      "loss: 0.088649 [4798464/10423404] 12:48:03 02/08/25:  56327 samples/second\n",
      "loss: 0.087027 [4998400/10423404] 12:48:06 02/08/25:  56253 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xag-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xag-targ.txt\n",
      "loss: 0.086321 [5198336/10423404] 12:48:10 02/08/25:  56227 samples/second\n",
      "loss: 0.085976 [5398272/10423404] 12:48:13 02/08/25:  56222 samples/second\n",
      "loss: 0.086083 [5598208/10423404] 12:48:17 02/08/25:  56229 samples/second\n",
      "loss: 0.086108 [5798144/10423404] 12:48:21 02/08/25:  56232 samples/second\n",
      "loss: 0.086193 [5998080/10423404] 12:48:24 02/08/25:  56253 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xah-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xah-targ.txt\n",
      "loss: 0.087924 [6198016/10423404] 12:48:28 02/08/25:  56231 samples/second\n",
      "loss: 0.089459 [6397952/10423404] 12:48:31 02/08/25:  56285 samples/second\n",
      "loss: 0.090716 [6597888/10423404] 12:48:35 02/08/25:  56294 samples/second\n",
      "loss: 0.090498 [6797824/10423404] 12:48:38 02/08/25:  56287 samples/second\n",
      "loss: 0.089945 [6997760/10423404] 12:48:42 02/08/25:  56285 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xai-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xai-targ.txt\n",
      "loss: 0.089155 [7197696/10423404] 12:48:45 02/08/25:  56260 samples/second\n",
      "loss: 0.088950 [7397632/10423404] 12:48:49 02/08/25:  56236 samples/second\n",
      "loss: 0.087803 [7597568/10423404] 12:48:53 02/08/25:  56192 samples/second\n",
      "loss: 0.087138 [7797504/10423404] 12:48:56 02/08/25:  56153 samples/second\n",
      "loss: 0.086322 [7997440/10423404] 12:49:00 02/08/25:  56129 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xaj-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xaj-targ.txt\n",
      "loss: 0.085845 [8197376/10423404] 12:49:03 02/08/25:  56115 samples/second\n",
      "loss: 0.085477 [8397312/10423404] 12:49:07 02/08/25:  56121 samples/second\n",
      "loss: 0.085565 [8597248/10423404] 12:49:11 02/08/25:  56111 samples/second\n",
      "loss: 0.085211 [8797184/10423404] 12:49:14 02/08/25:  56116 samples/second\n",
      "loss: 0.084958 [8997120/10423404] 12:49:18 02/08/25:  56094 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xak-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xak-targ.txt\n",
      "loss: 0.084715 [9197056/10423404] 12:49:21 02/08/25:  56106 samples/second\n",
      "loss: 0.084485 [9396992/10423404] 12:49:25 02/08/25:  56096 samples/second\n",
      "loss: 0.084133 [9596928/10423404] 12:49:29 02/08/25:  56072 samples/second\n",
      "loss: 0.083125 [9796864/10423404] 12:49:32 02/08/25:  56043 samples/second\n",
      "loss: 0.082414 [9996800/10423404] 12:49:36 02/08/25:  56008 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xal-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xal-targ.txt\n",
      "loss: 0.081874 [10196736/10423404] 12:49:40 02/08/25:  55983 samples/second\n",
      "loss: 0.081038 [10396672/10423404] 12:49:43 02/08/25:  55970 samples/second\n",
      "Epoch loss: 0.080957 [10423404/10423404] 12:49:44 02/08/25\n",
      "Saved PyTorch Model State to save-sgd-1.pth\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\test\\xaa-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\test\\xaa-targ.txt\n",
      "Test Error Avg loss: 0.088015 \n",
      "\n",
      "201 seconds per epoch - 201 seconds remaining\n",
      "-------------------------------\n",
      "Epoch 3 from 3\n",
      "-------------------------------\n",
      "Train on cuda with 10423404 positions\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xab-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xab-targ.txt\n",
      "loss: 0.083369 [ 199936/10423404] 12:50:03 02/08/25:  55869 samples/second\n",
      "loss: 0.092147 [ 399872/10423404] 12:50:06 02/08/25:  55932 samples/second\n",
      "loss: 0.091325 [ 599808/10423404] 12:50:10 02/08/25:  55911 samples/second\n",
      "loss: 0.091375 [ 799744/10423404] 12:50:13 02/08/25:  55906 samples/second\n",
      "loss: 0.087696 [ 999680/10423404] 12:50:17 02/08/25:  55940 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xac-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xac-targ.txt\n",
      "loss: 0.089472 [1199616/10423404] 12:50:20 02/08/25:  55986 samples/second\n",
      "loss: 0.088952 [1399552/10423404] 12:50:24 02/08/25:  56162 samples/second\n",
      "loss: 0.090118 [1599488/10423404] 12:50:27 02/08/25:  56136 samples/second\n",
      "loss: 0.091713 [1799424/10423404] 12:50:31 02/08/25:  56134 samples/second\n",
      "loss: 0.093318 [1999360/10423404] 12:50:35 02/08/25:  56178 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xad-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xad-targ.txt\n",
      "loss: 0.094865 [2199296/10423404] 12:50:38 02/08/25:  56264 samples/second\n",
      "loss: 0.095286 [2399232/10423404] 12:50:42 02/08/25:  56334 samples/second\n",
      "loss: 0.096656 [2599168/10423404] 12:50:45 02/08/25:  56397 samples/second\n",
      "loss: 0.098632 [2799104/10423404] 12:50:49 02/08/25:  56436 samples/second\n",
      "loss: 0.101256 [2999040/10423404] 12:50:52 02/08/25:  56548 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xae-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xae-targ.txt\n",
      "loss: 0.102618 [3198976/10423404] 12:50:55 02/08/25:  56602 samples/second\n",
      "loss: 0.102249 [3398912/10423404] 12:50:59 02/08/25:  56619 samples/second\n",
      "loss: 0.100880 [3598848/10423404] 12:51:03 02/08/25:  56568 samples/second\n",
      "loss: 0.099251 [3798784/10423404] 12:51:06 02/08/25:  56460 samples/second\n",
      "loss: 0.096964 [3998720/10423404] 12:51:10 02/08/25:  56462 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xaf-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xaf-targ.txt\n",
      "loss: 0.095141 [4198656/10423404] 12:51:13 02/08/25:  56410 samples/second\n",
      "loss: 0.092704 [4398592/10423404] 12:51:17 02/08/25:  56358 samples/second\n",
      "loss: 0.090312 [4598528/10423404] 12:51:21 02/08/25:  56274 samples/second\n",
      "loss: 0.088627 [4798464/10423404] 12:51:24 02/08/25:  56096 samples/second\n",
      "loss: 0.087006 [4998400/10423404] 12:51:28 02/08/25:  56058 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xag-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xag-targ.txt\n",
      "loss: 0.086300 [5198336/10423404] 12:51:32 02/08/25:  56079 samples/second\n",
      "loss: 0.085955 [5398272/10423404] 12:51:35 02/08/25:  56048 samples/second\n",
      "loss: 0.086063 [5598208/10423404] 12:51:39 02/08/25:  56115 samples/second\n",
      "loss: 0.086088 [5798144/10423404] 12:51:42 02/08/25:  56109 samples/second\n",
      "loss: 0.086174 [5998080/10423404] 12:51:46 02/08/25:  56158 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xah-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xah-targ.txt\n",
      "loss: 0.087904 [6198016/10423404] 12:51:49 02/08/25:  56215 samples/second\n",
      "loss: 0.089439 [6397952/10423404] 12:51:53 02/08/25:  56240 samples/second\n",
      "loss: 0.090696 [6597888/10423404] 12:51:56 02/08/25:  56202 samples/second\n",
      "loss: 0.090477 [6797824/10423404] 12:52:00 02/08/25:  56239 samples/second\n",
      "loss: 0.089925 [6997760/10423404] 12:52:03 02/08/25:  56285 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xai-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xai-targ.txt\n",
      "loss: 0.089136 [7197696/10423404] 12:52:07 02/08/25:  56287 samples/second\n",
      "loss: 0.088931 [7397632/10423404] 12:52:10 02/08/25:  56285 samples/second\n",
      "loss: 0.087784 [7597568/10423404] 12:52:14 02/08/25:  56293 samples/second\n",
      "loss: 0.087119 [7797504/10423404] 12:52:18 02/08/25:  56267 samples/second\n",
      "loss: 0.086304 [7997440/10423404] 12:52:21 02/08/25:  56232 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xaj-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xaj-targ.txt\n",
      "loss: 0.085827 [8197376/10423404] 12:52:25 02/08/25:  56225 samples/second\n",
      "loss: 0.085459 [8397312/10423404] 12:52:28 02/08/25:  56213 samples/second\n",
      "loss: 0.085548 [8597248/10423404] 12:52:32 02/08/25:  56228 samples/second\n",
      "loss: 0.085194 [8797184/10423404] 12:52:35 02/08/25:  56226 samples/second\n",
      "loss: 0.084941 [8997120/10423404] 12:52:39 02/08/25:  56223 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xak-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xak-targ.txt\n",
      "loss: 0.084698 [9197056/10423404] 12:52:43 02/08/25:  56210 samples/second\n",
      "loss: 0.084468 [9396992/10423404] 12:52:46 02/08/25:  56194 samples/second\n",
      "loss: 0.084117 [9596928/10423404] 12:52:50 02/08/25:  56214 samples/second\n",
      "loss: 0.083109 [9796864/10423404] 12:52:53 02/08/25:  56181 samples/second\n",
      "loss: 0.082399 [9996800/10423404] 12:52:57 02/08/25:  56158 samples/second\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\train\\xal-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\train\\xal-targ.txt\n",
      "loss: 0.081858 [10196736/10423404] 12:53:01 02/08/25:  56133 samples/second\n",
      "loss: 0.081023 [10396672/10423404] 12:53:04 02/08/25:  56111 samples/second\n",
      "Epoch loss: 0.080942 [10423404/10423404] 12:53:05 02/08/25\n",
      "Saved PyTorch Model State to save-sgd-2.pth\n",
      "Open feature file C:\\data\\extract\\2025\\beenine\\test\\xaa-feat.txt\n",
      "Open target  file C:\\data\\extract\\2025\\beenine\\test\\xaa-targ.txt\n",
      "Test Error Avg loss: 0.088006 \n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "save_prefix = 'save-sgd'\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "train_pos = 0\n",
    "start = time.time()\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1} from {epochs}\\n-------------------------------\")\n",
    "    train_pos, train_loss = train(device, train_dataloader, model, loss_fn, optimizer, train_pos)\n",
    "    train_losses.append(train_loss)\n",
    "    save_name = f\"{save_prefix}-{t}.pth\"\n",
    "    torch.save(model.state_dict(), save_name)\n",
    "    print(f\"Saved PyTorch Model State to {save_name}\")\n",
    "    test_loss = test(device, test_dataloader, model, loss_fn)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    tdiff = time.time() - start\n",
    "    spe = tdiff / (t + 1)\n",
    "    rem = round((epochs - t - 1) * spe)\n",
    "    if t + 1 < epochs:\n",
    "        spe = round(spe)\n",
    "        print(f\"{spe} seconds per epoch - {rem} seconds remaining\\n-------------------------------\")\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c7c3758-1a51-45a1-8d61-b5c1d60bfeb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.99851959, 0.99833253])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_losses) / train_losses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87689750-1147-431d-9153-1bacd6d1e2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.99978238, 0.99968236])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_losses) / test_losses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3875be0f-cc71-4b87-929a-0c9d9dc51918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[ 0.0174, -0.0100, -0.0172,  ...,  0.0074,  0.0218,  0.0305],\n",
       "           [-0.0275,  0.0294,  0.0093,  ..., -0.0183,  0.0007, -0.0233],\n",
       "           [ 0.0324,  0.0100, -0.0281,  ...,  0.0318, -0.0152,  0.0088],\n",
       "           ...,\n",
       "           [-0.0116, -0.0066, -0.0237,  ...,  0.0131, -0.0314, -0.0262],\n",
       "           [-0.0125, -0.0133, -0.0318,  ..., -0.0158,  0.0224,  0.0326],\n",
       "           [ 0.0181,  0.0057, -0.0040,  ...,  0.0045, -0.0048,  0.0069]],\n",
       "          device='cuda:0', requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([-0.0133, -0.0363, -0.0033, -0.0868, -0.0398, -0.0707, -0.0663, -0.0546,\n",
       "           -0.0270, -0.0715, -0.0158, -0.0541, -0.0571, -0.0424,  0.0350, -0.0542,\n",
       "           -0.0053, -0.0452, -0.0182, -0.0236, -0.0102, -0.0469,  0.0252,  0.0782,\n",
       "           -0.0688, -0.0534,  0.0032, -0.0233, -0.0799, -0.0285, -0.0124,  0.0183],\n",
       "          device='cuda:0', requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[ 0.0072,  0.0309,  0.0080, -0.0695,  0.0425, -0.0724,  0.0979, -0.0688,\n",
       "             0.0137,  0.0610,  0.0075, -0.0600,  0.0775,  0.1053,  0.0079, -0.0575,\n",
       "             0.0172, -0.0022, -0.0011,  0.0488,  0.0067, -0.0141,  0.0112,  0.0243,\n",
       "            -0.0315, -0.0278,  0.0099,  0.0400,  0.1071, -0.0174,  0.0798,  0.0156]],\n",
       "          device='cuda:0', requires_grad=True)],\n",
       "  'lr': 0.01,\n",
       "  'momentum': 0,\n",
       "  'dampening': 0,\n",
       "  'weight_decay': 0,\n",
       "  'nesterov': False,\n",
       "  'maximize': False,\n",
       "  'foreach': None,\n",
       "  'differentiable': False,\n",
       "  'fused': None}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "973f15b1-915f-4ecf-a95f-05e3d16e8953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0174, -0.0100, -0.0172,  ...,  0.0074,  0.0218,  0.0305],\n",
       "        [-0.0275,  0.0294,  0.0093,  ..., -0.0183,  0.0007, -0.0233],\n",
       "        [ 0.0324,  0.0100, -0.0281,  ...,  0.0318, -0.0152,  0.0088],\n",
       "        ...,\n",
       "        [-0.0116, -0.0066, -0.0237,  ...,  0.0131, -0.0314, -0.0262],\n",
       "        [-0.0125, -0.0133, -0.0318,  ..., -0.0158,  0.0224,  0.0326],\n",
       "        [ 0.0181,  0.0057, -0.0040,  ...,  0.0045, -0.0048,  0.0069]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups[0]['params'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
